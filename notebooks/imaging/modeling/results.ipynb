{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Results\n",
        "=================\n",
        "\n",
        "After fitting strong lens data a search returns a `result` variable, which we have used sparingly throughout the\n",
        "examples scripts to plot the maximum log likelihood tracer and fits. However, this `Result` object has a lot more\n",
        "information than that, and this script will cover everything it contains.\n",
        "\n",
        "This script uses the result generated in the script `autolens_workspace/notebooks/imaging/modeling/mass_total__source_parametric.py`.\n",
        "If you have not run the script or its results are not present in the output folder, the model-fit will be performed\n",
        "again to create the results.\n",
        "\n",
        "This model-fit fits the strong lens `Imaging` data with:\n",
        "\n",
        " - An `EllIsothermal` `MassProfile` for the lens galaxy's mass.\n",
        " - An `EllSersic` `LightProfile` for the source galaxy's light."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below, which we have omitted comments from, reperforms all the tasks that create the search and perform the\n",
        "model-fit in this script. If anything in this code is not clear to you, you should go over the beginner model-fit\n",
        "script again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"mass_sie__source_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", \"no_lens_light\", dataset_name)\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=path.join(dataset_path, \"image.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=imaging.shape_native, pixel_scales=imaging.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "imaging = imaging.apply_mask(mask=mask)\n",
        "\n",
        "model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.EllIsothermal),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.EllSersic),\n",
        "    )\n",
        ")\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"imaging\", \"modeling\"),\n",
        "    name=\"mass[sie]_source[bulge]\",\n",
        "    unique_tag=dataset_name,\n",
        "    nlive=50,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=imaging)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, so we have the `Result` object we'll cover in this script. As a reminder, we can use the \n",
        "`max_log_likelihood_tracer` and `max_log_likelihood_fit` to plot the results of the fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=mask.masked_grid_sub_1\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "fit_imaging_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_imaging_plotter.subplot_fit_imaging()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains a lot more information about the model-fit. \n",
        "\n",
        "For example, its `Samples` object contains the complete set of non-linear search samples, for example every set of \n",
        "parameters evaluated, their log likelihoods and so on, which are used for computing information about the model-fit \n",
        "such as the error on every parameter. Our model-fit used the nested sampling algorithm Dynesty, so the `Samples` object\n",
        "returned is a `NestSamples` objct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Nest Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class contains all the parameter samples, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameters[0])\n",
        "print(\"The fourth parameter of the tenth sample\")\n",
        "print(samples.parameters[9][3])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class contains the log likelihood, log prior, log posterior and weights of every sample, where:\n",
        "\n",
        "   - The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise \n",
        "     normalization).\n",
        "\n",
        "   - The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log\n",
        "     posterior value.\n",
        "\n",
        "   - The log posterior is log_likelihood + log_prior.\n",
        "\n",
        "   - The weight gives information on how samples should be combined to estimate the posterior. The weight values \n",
        "     depend on the sampler used. For example for an MCMC search they will all be 1`s whereas for the nested sampling\n",
        "     method used in this example they are weighted as a combination of the log likelihood value and prior.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihoods[9])\n",
        "print(samples.log_priors[9])\n",
        "print(samples.log_posteriors[9])\n",
        "print(samples.weights[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` contain the maximum log likelihood model of the fit (we actually used this when we used the \n",
        "max_log_likelihood_tracer and max_log_likelihood_fit properties of the results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_vector = samples.max_log_likelihood_vector\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(ml_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides us with a list of all model parameters. However, this isn't that much use, which values correspond to \n",
        "which parameters?\n",
        "\n",
        "The list of parameter names are available as a property of the `Samples`, as are parameter labels which can be used \n",
        "for labeling figures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(samples.model.model_component_and_parameter_names)\n",
        "print(samples.model.parameter_labels)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These lists will be used later for visualization, however it can be more useful to create the model instance of every \n",
        "fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_instance = samples.max_log_likelihood_instance\n",
        "print(\"Maximum Log Likelihood Model Instance: \\n\")\n",
        "print(ml_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance contains all the model components of our fit, most importantly the list of galaxies we specified in \n",
        "the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(ml_instance.galaxies)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(ml_instance.galaxies.lens)\n",
        "print(ml_instance.galaxies.source)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(ml_instance.galaxies.lens.mass)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this list of galaxies to create the maximum log likelihood `Tracer`, which, funnily enough, \n",
        "is the property of the result we've used up to now!\n",
        "\n",
        "(If we had the `Imaging` available we could easily use this to create the maximum log likelihood `FitImaging`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_tracer = al.Tracer.from_galaxies(galaxies=ml_instance.galaxies)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(tracer=ml_tracer, grid=mask.unmasked_grid_sub_1)\n",
        "tracer_plotter.subplot_tracer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access the `median pdf` model, which is the model computed by marginalizing over the samples of every \n",
        "parameter in 1D and taking the median of this PDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_vector = samples.median_pdf_vector\n",
        "mp_instance = samples.median_pdf_instance\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists: \\n\")\n",
        "print(mp_vector, \"\\n\")\n",
        "print(\"Most probable Model Instances: \\n\")\n",
        "print(mp_instance, \"\\n\")\n",
        "print(mp_instance.galaxies.lens.mass)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the model parameters at a given sigma value (e.g. at 3.0 sigma limits).\n",
        "\n",
        "These parameter values do not account for covariance between the model. For example if two parameters are degenerate \n",
        "this will find their values from the degeneracy in the `same direction` (e.g. both will be positive). we'll cover\n",
        "how to handle covariance elsewhere.\n",
        "\n",
        "Here, I use \"uv3\" to signify this is an upper value at 3 sigma confidence,, and \"lv3\" for the lower value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uv3_vector = samples.vector_at_upper_sigma(sigma=3.0)\n",
        "uv3_instance = samples.instance_at_upper_sigma(sigma=3.0)\n",
        "lv3_vector = samples.vector_at_lower_sigma(sigma=3.0)\n",
        "lv3_instance = samples.instance_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Lists: \\n\")\n",
        "print(uv3_vector, \"\\n\")\n",
        "print(lv3_vector, \"\\n\")\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(uv3_instance, \"\\n\")\n",
        "print(lv3_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the upper and lower errors on each parameter at a given sigma limit.\n",
        "\n",
        "Here, \"ue3\" signifies the upper error at 3 sigma. \n",
        "\n",
        "( Need to fix bug, sigh)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ue3_vector = samples.error_vector_at_upper_sigma(sigma=3.0)\n",
        "# ue3_instance = samples.error_instance_at_upper_sigma(sigma=3.0)\n",
        "# le3_vector = samples.error_vector_at_lower_sigma(sigma=3.0)\n",
        "# le3_instance = samples.error_instance_at_lower_sigma(sigma=3.0)\n",
        "#\n",
        "# print(\"Errors Lists: \\n\")\n",
        "# print(ue3_vector, \"\\n\")\n",
        "# print(le3_vector, \"\\n\")\n",
        "# print(\"Errors Instances: \\n\")\n",
        "# print(ue3_instance, \"\\n\")\n",
        "# print(le3_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The maximum log likelihood of each model fit and its Bayesian log evidence (estimated via the nested sampling \n",
        "algorithm) are also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood and Log Evidence: \\n\")\n",
        "print(max(samples.log_likelihoods))\n",
        "print(samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the library:\n",
        "\n",
        " corner.py: https://corner.readthedocs.io/en/latest/\n",
        "\n",
        "(In built visualization for PDF's and non-linear searches is a future feature of PyAutoFit, but for now you`ll have to \n",
        "use the libraries yourself!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import corner\n",
        "\n",
        "corner.corner(\n",
        "    xs=samples.parameters,\n",
        "    weights=samples.weights,\n",
        "    labels=samples.model.parameter_labels,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator__\n",
        "\n",
        "Once a search has completed running, we have a set of results on our hard disk we manually inspect and analyse. \n",
        "Alternatively, we return the results from the search.run() method and manipulate them in a Python script, as we did\n",
        "in this script.\n",
        "\n",
        "However, imagine your dataset is large and consists of many images of strong lenses. You analyse each image \n",
        "individually using the same search, producing a large set of results on your hard disk corresponding to the full sample.\n",
        "That will be a lot of paths and directories to navigate! At some point, there`ll be too many results for it to be\n",
        "a sensible use of your time to analyse the results by sifting through the outputs on your hard disk.\n",
        "\n",
        "PyAutoFit`s aggregator tool allows us to load results in a Python script or, more importantly, a Jupyter notebook. This\n",
        "bypasses the need for us to run a search and can load the results of any number of lenses at once, allowing us to \n",
        "manipulate the results of extremely large lens samples!\n",
        "\n",
        "If the `Aggregator`. sounds useful to you, then checkout the tutorials in the path:\n",
        "\n",
        " `autolens_workspace/advanced/aggregator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}