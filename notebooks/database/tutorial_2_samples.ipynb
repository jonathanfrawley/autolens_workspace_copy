{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Database 1: Samples\n",
        "===================\n",
        "\n",
        "After fitting a large suite of strong lens data, we can use the aggregator to load the database's results. We can then\n",
        "manipulate, interpret and visualize them using a Python script or Jupyter notebook.\n",
        "\n",
        "This script uses the results generated by the script `/autolens_workspace/database/tutorial_0_model_fits.py`, which\n",
        "fitted 3 simulated strong lenses with:\n",
        "\n",
        " - An `EllIsothermal` `MassProfile` for the lens galaxy's mass.\n",
        " - An `EllSersic` `LightProfile` representing a bulge for the source galaxy's light.\n",
        "\n",
        "__Samples__\n",
        "\n",
        "This script covers how to manipulate the `Samples` object returned from a *PyAutoLens* model-fit, which you have most\n",
        "likely already encountered when analysing the results of a model-fit. Nevertheless, we'll also learn how to use the\n",
        "`Aggregator`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load the results in the `output` folder into a sqlite database using the `Aggregator`. We simply point to the \n",
        "path where we want the database to be created and add the directory `autolens_workspace/output/database`.\n",
        "\n",
        "Checkout the output folder, you should see a `database.sqlite` file which contains the model-fits to the 3 `Gaussian`\n",
        "datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# from autofit.database.aggregator import Aggregator\n",
        "#\n",
        "# database_file = path.join(\"output\", \"database\", \"database.sqlite\")\n",
        "#\n",
        "# if path.isfile(database_file):\n",
        "#     os.remove(database_file)\n",
        "#\n",
        "# agg = Aggregator.from_database(path.join(database_file))\n",
        "# agg.add_directory(path.join(\"output\", \"database\"))\n",
        "\n",
        "agg = af.Aggregator(directory=path.join(\"output\", \"database\"))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before using the aggregator to inspect results, let me quickly cover Python generators. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all of the objects that it loads from the database \n",
        "as generators (as opposed to a list, or dictionary, or other Python type).\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, this will use \n",
        "a lot of memory and crash your laptop! On the other hand, a generator only stores the object in memory when it is used; \n",
        "Python is then free to overwrite it afterwards. Thus, your laptop won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        " 1) A generator has no length and to determine how many entries it contains you first must turn it into a list.\n",
        "\n",
        " 2) Once we use a generator, we cannot use it again and need to remake it. For this reason, we typically avoid \n",
        " storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a `samples` generator of every fit. As we saw in the `result.py` example scripts, an instance of \n",
        "the `Samples` class acts as an interface to the results of the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this the length of this generator converted to a list of outputs we see 3 different NestSamples \n",
        "instances. These correspond to each fit of each search to each of our 3 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"NestedSampler Samples: \\n\")\n",
        "print(samples_gen)\n",
        "print()\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class contains all the parameter samples, which is a list of lists where:\n",
        " \n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameters[0])\n",
        "    print(\"The third parameter of the tenth sample\")\n",
        "    print(samples.parameters[9][2])\n",
        "\n",
        "print(\"Samples: \\n\")\n",
        "print(agg.values(\"samples\"))\n",
        "print()\n",
        "print(\"Total Samples Objects = \", len(list(agg.values(\"samples\"))), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class contains the log likelihood, log prior, log posterior and weights of every sample, where:\n",
        "\n",
        " - The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise  \n",
        " normalization).\n",
        "    \n",
        " - The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log\n",
        " posterior value.\n",
        "      \n",
        " - The log posterior is log_likelihood + log_prior.\n",
        "    \n",
        " - The weight gives information on how samples should be combined to estimate the posterior. The weight values \n",
        " depend on the sampler used, for example for MCMC they will all be 1`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "    print(samples.log_likelihoods[9])\n",
        "    print(samples.log_priors[9])\n",
        "    print(samples.log_posteriors[9])\n",
        "    print(samples.weights[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the outputs to create a list of the maximum log likelihood model of each fit to our three images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_vector = [samps.max_log_likelihood_vector for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Max Log Likelihood Model Parameter Lists: \\n\")\n",
        "print(ml_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides us with lists of all model parameters. However, this isn't that much use, which values correspond to \n",
        "which parameters?\n",
        "\n",
        "The list of parameter names are available as a property of the `Model` included with the `Samples`, as are labels \n",
        "which can be used for labeling figures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    model = samples.model\n",
        "    print(model)\n",
        "    print(model.parameter_names)\n",
        "    print(model.parameter_labels)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These lists will be used later for visualization, how it is often more useful to create the model instance of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_instances = [samps.max_log_likelihood_instance for samps in agg.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Model Instances: \\n\")\n",
        "print(ml_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance contains all the model components of our fit, most importantly the list of galaxies we specified in \n",
        "the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(ml_instances[0].galaxies)\n",
        "print(ml_instances[1].galaxies)\n",
        "print(ml_instances[2].galaxies)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These galaxies will be named according to the search (in this case, `lens` and `source`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(ml_instances[0].galaxies.lens)\n",
        "print()\n",
        "print(ml_instances[1].galaxies.source)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Their `LightProfile`'s and `MassProfile`'s are also named according to the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(ml_instances[0].galaxies.lens.mass)\n",
        "print(ml_instances[1].galaxies.source.bulge)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the `median pdf` model, which is the model computed by marginalizing over the samples of every parameter \n",
        "in 1D and taking the median of this PDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_vector = [samps.median_pdf_vector for samps in agg.values(\"samples\")]\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists: \\n\")\n",
        "print(mp_vector, \"\\n\")\n",
        "print(\"Most probable Model Instances: \\n\")\n",
        "print(mp_instances, \"\\n\")\n",
        "print(mp_instances[0].galaxies.lens.mass)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the model parameters at a given sigma value (e.g. at 3.0 sigma limits).\n",
        "\n",
        "These parameter values do not account for covariance between the model. For example if two parameters are degenerate \n",
        "this will find their values from the degeneracy in the `same direction` (e.g. both will be positive). we'll cover\n",
        "how to handle covariance in a later tutorial.\n",
        "\n",
        "Here, I use \"uv3\" to signify this is an upper value at 3 sigma confidence,, and \"lv3\" for the lower value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uv3_vectors = [\n",
        "    samps.vector_at_upper_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "uv3_instances = [\n",
        "    samps.instance_at_upper_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "lv3_vectors = [\n",
        "    samps.vector_at_lower_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "lv3_instances = [\n",
        "    samps.instance_at_lower_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Errors Lists: \\n\")\n",
        "print(uv3_vectors, \"\\n\")\n",
        "print(lv3_vectors, \"\\n\")\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(uv3_instances, \"\\n\")\n",
        "print(lv3_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the upper and lower errors on each parameter at a given sigma limit.\n",
        "\n",
        "Here, \"ue3\" signifies the upper error at 3 sigma. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue3_vectors = [\n",
        "    samps.error_vector_at_upper_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "ue3_instances = [\n",
        "    samps.error_instance_at_upper_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "le3_vectors = [\n",
        "    samps.error_vector_at_lower_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    samps.error_instance_at_lower_sigma(sigma=3.0) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Errors Lists: \\n\")\n",
        "print(ue3_vectors, \"\\n\")\n",
        "print(le3_vectors, \"\\n\")\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(ue3_instances, \"\\n\")\n",
        "print(le3_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The maximum log likelihood of each model fit and its Bayesian log evidence (estimated via the nested sampling \n",
        "algorithm) are also available.\n",
        "\n",
        "Given each fit is to a different image, these are not very useful. However, in a later tutorial we'll look at using \n",
        "the aggregator for images that we fit with many different models and many different pipelines, in which case comparing \n",
        "the evidences allows us to perform Bayesian model comparison!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihoods and Log Evidences: \\n\")\n",
        "print([max(samps.log_likelihoods) for samps in agg.values(\"samples\")])\n",
        "print([samps.log_evidence for samps in agg.values(\"samples\")])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the \"model_results\" of all searches, which is string that summarizes every fit`s lens model providing \n",
        "quick inspection of all results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = agg.model_results\n",
        "print(\"Model Results Summary: \\n\")\n",
        "print(results, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the library:\n",
        "\n",
        " corner.py: https://corner.readthedocs.io/en/latest/\n",
        "\n",
        "(In built visualization for PDF's and non-linear searches is a future feature of PyAutoFit, but for now you`ll have to \n",
        "use the libraries yourself!).\n",
        "\n",
        "(uncomment the code below to make a corner.py plot.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import corner\n",
        "#\n",
        "# for samples in agg.values(\"samples\"):\n",
        "#\n",
        "#     corner.corner(\n",
        "#         xs=samples.parameters,\n",
        "#         weights=samples.weights,\n",
        "#         labels=samples.model.parameter_labels,\n",
        "#     )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}