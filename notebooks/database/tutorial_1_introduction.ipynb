{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database: Introduction\n",
    "======================\n",
    "\n",
    "The default behaviour of **PyAutoLens** is for model-fitting results to be output to hard-disc in folders, which are\n",
    "straight forward to navigate and manually check. For small model-fitting tasks this is sufficient, however many users \n",
    "have a need to perform many model fits to large sampels of lenses, making manual inspection of results time consuming.\n",
    "\n",
    "PyAutoLens's database feature outputs all model-fitting results as a\n",
    "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
    "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
    "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
    "loaded.\n",
    "\n",
    "This script fits a sample of three simulated strong lenses using the same non-linear search. The results will be used\n",
    "to illustrate the database in the database tutorials that follow.\n",
    "\n",
    "The search fits each lens with:\n",
    " \n",
    " - An `EllIsothermal` `MassProfile` for the lens galaxy's mass.\n",
    " - An `EllSersic` `LightProfile` for the source galaxy's light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T11:32:24.792071Z",
     "iopub.status.busy": "2021-04-10T11:32:24.791562Z",
     "iopub.status.idle": "2021-04-10T11:32:27.042956Z",
     "shell.execute_reply": "2021-04-10T11:32:27.042602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/autolens_workspace\n",
      "Working Directory has been set to `/mnt/c/Users/Jammy/Code/PyAuto/autolens_workspace`\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import json\n",
    "from os import path\n",
    "import autofit as af\n",
    "import autolens as al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
    "\n",
    "The 3 datasets are in the `autolens_workspace/dataset/database` folder.\n",
    "\n",
    "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
    "string to do this, so lets create a list of the 3 dataset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T11:32:27.045439Z",
     "iopub.status.busy": "2021-04-10T11:32:27.045134Z",
     "iopub.status.idle": "2021-04-10T11:32:27.047168Z",
     "shell.execute_reply": "2021-04-10T11:32:27.046852Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"mass_sie__source_sersic__0\",\n",
    "    \"mass_sie__source_sersic__1\",\n",
    "    \"mass_sie__source_sersic__2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the dataset type, label and name, which we use to determine the path we load the data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T11:32:27.052248Z",
     "iopub.status.busy": "2021-04-10T11:32:27.051929Z",
     "iopub.status.idle": "2021-04-10T11:32:30.255324Z",
     "shell.execute_reply": "2021-04-10T11:32:30.255584Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:database_example already completed, skipping non-linear search.\n",
      "INFO:root:database_example already completed, skipping non-linear search.\n",
      "INFO:root:database_example already completed, skipping non-linear search.\n"
     ]
    }
   ],
   "source": [
    "pixel_scales = 0.1\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    \"\"\"\n",
    "    __Paths__\n",
    "    \n",
    "    Set up the config and output paths.\n",
    "    \"\"\"\n",
    "    dataset_path = path.join(\"dataset\", \"database\", dataset_name)\n",
    "\n",
    "    \"\"\"\n",
    "    __Dataset__\n",
    "    \n",
    "    Using the dataset path, load the data (image, noise-map, PSF) as an `Imaging` object from .fits files.\n",
    "    \n",
    "    This `Imaging` object will be available via the aggregator. Note also that we give the dataset a `name` via the\n",
    "    command `name=dataset_name`. we'll use this name in the aggregator tutorials.\n",
    "    \"\"\"\n",
    "    imaging = al.Imaging.from_fits(\n",
    "        image_path=path.join(dataset_path, \"image.fits\"),\n",
    "        psf_path=path.join(dataset_path, \"psf.fits\"),\n",
    "        noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
    "        pixel_scales=pixel_scales,\n",
    "        name=dataset_name,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    __Mask__\n",
    "    \n",
    "    The `Mask2D` we fit this data-set with, which will be available via the aggregator.\n",
    "\n",
    "    The `SettingsImaging` (which customize the fit of the search`s fit), will also be available to the aggregator! \n",
    "    \"\"\"\n",
    "    mask = al.Mask2D.circular(\n",
    "        shape_native=imaging.shape_native, pixel_scales=imaging.pixel_scales, radius=3.0\n",
    "    )\n",
    "\n",
    "    settings_imaging = al.SettingsImaging(grid_class=al.Grid2D, sub_size=1)\n",
    "\n",
    "    imaging = imaging.apply_mask(mask=mask)\n",
    "    imaging = imaging.apply_settings(settings=settings_imaging)\n",
    "\n",
    "    \"\"\"\n",
    "    __Info__\n",
    "\n",
    "    Information about our model-fit that isn't part of the model-fit can be made accessible to the database, by \n",
    "    passing an `info` dictionary. \n",
    "\n",
    "    Below we load this info dictionary from an `info.json` file stored in each dataset's folder. This dictionary\n",
    "    contains the (hypothetical) lens redshift, source redshift and lens velocity dispersion of every lens in our sample.\n",
    "    \"\"\"\n",
    "    with open(path.join(dataset_path, \"info.json\")) as json_file:\n",
    "        info = json.load(json_file)\n",
    "\n",
    "    \"\"\"\n",
    "    __Pickle Files__\n",
    "\n",
    "    We can pass strings specifying the path and filename of .pickle files stored on our hard-drive to the `search.fit()`\n",
    "    method, which will make them accessible to the aggregator to aid interpretation of results. Our simulated strong\n",
    "    lens datasets have a `true_tracer.pickle` file which we pass in below, which we use in the `Aggregator` tutorials \n",
    "    to check if the model-fit recovers its true input parameters.\n",
    "    \"\"\"\n",
    "    pickle_files = [path.join(dataset_path, \"true_tracer.pickle\")]\n",
    "\n",
    "    \"\"\"\n",
    "    Model-Fit:\n",
    "    \n",
    "    We perform the model-fit as per usual\n",
    "    \"\"\"\n",
    "    model = af.Collection(\n",
    "        galaxies=af.Collection(\n",
    "            lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.EllIsothermal),\n",
    "            source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.EllSersic),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    search = af.DynestyStatic(\n",
    "        path_prefix=path.join(\"database\", dataset_name),\n",
    "        name=\"database_example\",\n",
    "        nlive=50,\n",
    "    )\n",
    "\n",
    "    analysis = al.AnalysisImaging(dataset=imaging)\n",
    "\n",
    "    search.fit(analysis=analysis, model=model, info=info, pickle_files=pickle_files)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
